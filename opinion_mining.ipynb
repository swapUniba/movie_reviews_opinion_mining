{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opinion_mining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmTaG-hFjNTc"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi5s2MfpjUI8"
      },
      "source": [
        "runtime_type = 'hosted' # 'local' if using local runtime, 'hosted' if using Colab runtime\n",
        "download_resources = True # set to True in order to download required resources, this is needed every time when using a hosted runtime and one time only if running locally"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAFas0ufpjKA"
      },
      "source": [
        "## Connessione a Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PnyatulpSfy"
      },
      "source": [
        "if runtime_type == 'hosted':\n",
        "  from google.colab import drive \n",
        "  drive.mount('/content/drive') # mount Google Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXqz_4idpo-q"
      },
      "source": [
        "## Selezione directory di lavoro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Hc0drLptod"
      },
      "source": [
        "# Google Drive working directory path\n",
        "if runtime_type == 'hosted':\n",
        "  %cd /content/drive/My\\ Drive/Colab\\ Files/ \n",
        "\n",
        "# local working directory path\n",
        "if runtime_type == 'local':\n",
        "  %cd D:\\opimi_test\\ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JU6pHeu9fL8"
      },
      "source": [
        "## Download risorse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1yO_AlEvtjn"
      },
      "source": [
        "if download_resources:\n",
        "  !pip install stanza\n",
        "  import stanza\n",
        "  import nltk\n",
        "  stanza.download('english')\n",
        "  nltk.download('wordnet')\n",
        "  nltk.download('sentiwordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiNUEdC1KBrQ"
      },
      "source": [
        "# Funzioni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6k5l4k5f6oT"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz6fRVRQf6oU"
      },
      "source": [
        "import pandas as pd\n",
        "import stanza\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def sentiment_analysis(df):\n",
        "    '''\n",
        "    Returns a DataFrame with the reviews splitted in tokens associated with the sentiment detected for the source sentence.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame on which to perform the sentiment analysis, it must have movie_id, review_id and review_text columns.\n",
        "\n",
        "    Returns:\n",
        "        reviewSentiment: The DataFrame with a sentiment tag for each token.\n",
        "    '''\n",
        "    \n",
        "    df = df.copy()\n",
        "    df['movie_id'] = df['movie_id'].astype(str)\n",
        "    df['review_id'] = df['review_id'].astype(str)\n",
        "    df['review_text'] = df['review_text'].astype(str)\n",
        "    \n",
        "    nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\n",
        "    rows_list = []\n",
        "    for row in df.itertuples():\n",
        "        doc = nlp(str(row.review_text))\n",
        "        for sentence in doc.sentences:\n",
        "            sentiment = sentiment_to_word(int(sentence.sentiment))\n",
        "            for token in sentence.tokens:\n",
        "                dict1 = {}\n",
        "                dict1.update({'movie_id': str(row.movie_id)})\n",
        "                dict1.update({'review_id': str(row.review_id)}) \n",
        "                dict1.update({'token': str(token.text)}) \n",
        "                dict1.update({'sentiment': sentiment})\n",
        "                rows_list.append(dict1)\n",
        "    sentiment_df = pd.DataFrame(rows_list)\n",
        "    return sentiment_df\n",
        "\n",
        "def sentiment_to_word(number):\n",
        "    '''\n",
        "    Converts the numerical representation of sentiment into a string.\n",
        "\n",
        "    Args:\n",
        "        number: Sentiment represented as a number.\n",
        "\n",
        "    Returns:\n",
        "        sentiment_string: Sentiment as a string.\n",
        "    '''\n",
        "    \n",
        "    switcher = {\n",
        "        0: 'negative',\n",
        "        1: 'neutral',\n",
        "        2: 'positive'\n",
        "    }\n",
        "    sentiment_string = (switcher.get(number, 'Invalid number specified'))\n",
        "    return sentiment_string\n",
        "\n",
        "def pos_lemma(df):\n",
        "    '''\n",
        "    Returns a DataFrame with each token associated with its universal POS (UPOS) tag, treebank-specific POS (XPOS) tag, \n",
        "    universal morphological features (UFeats) and lemma.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame with the sentences to POS-tag, it must be the output of sentiment_analysis.\n",
        "\n",
        "    Returns:\n",
        "        posTagging: The Dataframe with tokens associated to their POS-tags and lemmas.\n",
        "    '''\n",
        "    \n",
        "    df = df.copy()\n",
        "    df['movie_id'] = df['movie_id'].astype(str)\n",
        "    df['review_id'] = df['review_id'].astype(str)\n",
        "    df['token'] = df['token'].astype(str)\n",
        "\n",
        "    df = df.groupby(['movie_id', 'review_id'], as_index=False, sort=False)[['token']].agg(lambda x: ' '.join(x)) # group tokens by movie and review\n",
        "    \n",
        "    nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma', tokenize_pretokenized=True)\n",
        "    rows_list = []\n",
        "    for row in df.itertuples():\n",
        "        doc = nlp(row.token)\n",
        "        for sent in doc.sentences:\n",
        "            for word in sent.words:\n",
        "                dict1 = {}\n",
        "                dict1.update({'movie_id': str(row.movie_id)})\n",
        "                dict1.update({'review_id': str(row.review_id)})\n",
        "                dict1.update({'token': (str(word.lemma))})\n",
        "                dict1.update({'upos': str(word.upos)})\n",
        "                dict1.update({'xpos': str(word.xpos)})\n",
        "                dict1.update({'feats': str(word.feats if word.feats else \"_\")})\n",
        "                rows_list.append(dict1)\n",
        "    pos_lemma_df = pd.DataFrame(rows_list)\n",
        "    return pos_lemma_df\n",
        "\n",
        "def calculate_swn_score(df):\n",
        "    '''\n",
        "    Tags each token with its positivity, negativity and objectivity score from SentiWordNet\n",
        "\n",
        "    Args:\n",
        "        df: The dataframe with the tokens, it must be the output of pos_lemma.\n",
        "\n",
        "    Returns:\n",
        "        The dataframe with the tagged tokens.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    df['token'] = df['token'].astype(str)\n",
        "    df['xpos'] = df['xpos'].astype(str)\n",
        "    df['positivity'] = df.apply(lambda x: get_positivity_score(x.token, x.xpos), axis=1)\n",
        "    df['negativity'] = df.apply(lambda x: get_negativity_score(x.token, x.xpos), axis=1)\n",
        "    df['objectivity'] = df.apply(lambda x: get_objectivity_score(x.token, x.xpos), axis=1)\n",
        "    return df\n",
        "\n",
        "def get_positivity_score(word, xpos_tag):\n",
        "    '''\n",
        "    Returns the positivity score for the given word and POS tag.\n",
        "\n",
        "    Args:\n",
        "        word: The original word.\n",
        "        xpos_tag: The treebank-specific POS tag of the word.\n",
        "\n",
        "    Returns:\n",
        "        word.pos_score(): The positivity score of the given word.\n",
        "        0 if the given treebank-specific POS tag can't be mapped to a wordnet POS tag or if can't get a value for the given word.\n",
        "    '''\n",
        "    \n",
        "    pos_tag = xpos_to_wnpos(xpos_tag)\n",
        "    if pos_tag == None:\n",
        "        return 0\n",
        "    arg = str(word) + '.' + str(pos_tag) + '.' + '01'\n",
        "    try:\n",
        "        word = swn.senti_synset(arg)\n",
        "        return word.pos_score()\n",
        "    except:\n",
        "        return 0 # return positivity score 0 if can't find the lemma\n",
        "    \n",
        "def get_negativity_score(word, xpos_tag):\n",
        "    '''\n",
        "    Returns the negativity score for the given word and POS tag.\n",
        "\n",
        "    Args:\n",
        "        word: The original word.\n",
        "        xpos_tag: The treebank-specific POS tag of the word.\n",
        "\n",
        "    Returns:\n",
        "        word.pos_score(): The negativity score of the given word.\n",
        "        0 if the given treebank-specific POS tag can't be mapped to a wordnet POS tag or if can't get a value for the given word.\n",
        "    '''\n",
        "\n",
        "    pos_tag = xpos_to_wnpos(xpos_tag)\n",
        "    if pos_tag == None:\n",
        "        return 0\n",
        "    arg = str(word) + '.' + pos_tag + '.' + '01'\n",
        "    try:\n",
        "        word = swn.senti_synset(arg)\n",
        "        return word.neg_score()\n",
        "    except:\n",
        "        return 0 # return positivity score 0 if can't find the lemma\n",
        "    \n",
        "def get_objectivity_score(word, xpos_tag):\n",
        "    '''\n",
        "    Returns the objectivity score for the given word and POS tag.\n",
        "\n",
        "    Args:\n",
        "        word: The original word.\n",
        "        xpos_tag: The treebank-specific POS tag of the word.\n",
        "\n",
        "    Returns:\n",
        "        word.pos_score(): The objectivity score of the given word. \n",
        "        0 if the given treebank-specific POS tag can't be mapped to a wordnet POS tag or if can't get a value for the given word.\n",
        "    '''\n",
        "\n",
        "    pos_tag = xpos_to_wnpos(xpos_tag)\n",
        "    if pos_tag == None:\n",
        "        return 0\n",
        "    arg = str(word) + '.' + pos_tag + '.' + '01'\n",
        "    try:\n",
        "        word = swn.senti_synset(arg)\n",
        "        return word.obj_score()\n",
        "    except:\n",
        "        return 0 # return positivity score 0 if can't find the lemma\n",
        "        \n",
        "def xpos_to_wnpos(xpos_tag):\n",
        "    '''\n",
        "    Returns wordnet POS tag for the given treebank-specific POS tag or None if can't convert it.\n",
        "\n",
        "    Args:\n",
        "        xpos_tag: The xpos tag.\n",
        "\n",
        "    Returns:\n",
        "        tag_dict.get(): Tag accepted by lemmatize().\n",
        "    '''\n",
        "\n",
        "    xpos_tag = xpos_tag[0].upper()\n",
        "    tag_dict = {\"J\": wn.ADJ,\n",
        "                \"N\": wn.NOUN,\n",
        "                \"V\": wn.VERB,\n",
        "                \"R\": wn.ADV}\n",
        "\n",
        "    return tag_dict.get(xpos_tag, None)\n",
        "\n",
        "def filter_by_sentiment(df, sentiment):\n",
        "    '''\n",
        "    Returns the given DataFrame after removing tokens not matching the given sentiment.\n",
        "\n",
        "    Args:\n",
        "        df: The original dataframe with tokens associated to the sentiment of the source sentence.\n",
        "        sentiment: The desired sentiment to keep.\n",
        "\n",
        "    Returns:\n",
        "        df: The DataFrame containing only tokens associated to the given sentiment.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    df['sentiment'] = df['sentiment'].astype(str)\n",
        "    df.drop(df[(df.sentiment != sentiment)].index, inplace=True) # drop tokens not matching the desired sentiment\n",
        "    del df['sentiment'] # delete now useless sentiment column\n",
        "    return df\n",
        "\n",
        "def lowercase(df):\n",
        "    '''\n",
        "    Returns the given DataFrame after lowercasing all tokens.\n",
        "\n",
        "    Args:\n",
        "        df: The original dataframe.\n",
        "\n",
        "    Returns:\n",
        "        df: The DataFrame containing only lowercased tokens.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    df['token'] = df['token'].astype(str)\n",
        "    df['token'] = df['token'].map(lambda x: x.lower())\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U99f8bt0f6oi"
      },
      "source": [
        "## Aspect Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osYcd-GSf6oj"
      },
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "\n",
        "def extract_tokens(df):\n",
        "    '''\n",
        "    Returns a DataFrame with filtered tokens. Tokens are lowercased and filtered removing stopwords from nltk, non alphabetic characters, singular and plural proper nouns,\n",
        "    comparative and superlative adjectives and any other token that is not an adjective or a noun.\n",
        "    \n",
        "    Args:\n",
        "        df: The DataFrame with the pos-tagged tokens to filter, it must be the output of pos_tag.\n",
        "\n",
        "    Returns:\n",
        "        tokenization: The Dataframe with a tokens list associated to each movie id.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    df['movie_id'] = df['movie_id'].astype(str)\n",
        "    df['review_id'] = df['review_id'].astype(str)\n",
        "    df['token'] = df['token'].astype(str)\n",
        "    df['upos'] = df['upos'].astype(str)\n",
        "    df['xpos'] = df['xpos'].astype(str)\n",
        "    df['feats'] = df['feats'].astype(str)\n",
        "    df['positivity'] = df['positivity'].astype(float)\n",
        "    df['negativity'] = df['negativity'].astype(float)\n",
        "    df['objectivity'] = df['objectivity'].astype(float)\n",
        "    \n",
        "    # Filter tokens using pos-tags\n",
        "    df.drop(df[(df.upos == 'PUNCT')].index, inplace=True) # drop punctuation\n",
        "    df.drop(df[(df.upos == 'PROPN')].index, inplace=True) # drop tokens that are proper nouns\n",
        "    df.drop(df[(df.xpos == 'JJR') | (df.xpos == 'JJS')].index, inplace=True) # drop tokens that are comparative or superlative adjectives\n",
        "    df.drop(df[~((df.xpos == 'JJ') | (df.upos == 'NOUN'))].index, inplace=True) # drop tokens that are not adjectives or nouns\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    # Clean tokens from special characters and numbers\n",
        "    df = alphabetize_tokens(df)\n",
        "    df.drop(df[(df.token == '')].index, inplace=True) # drop empty tokens resulted from alphabetization\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    df.drop(df[(df.objectivity + df.positivity + df.negativity == 0.0)].index, inplace=True)\n",
        "    \n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    df = remove_stopwords(df, 'token')\n",
        "    \n",
        "    # Join tokens into a list and return\n",
        "    df = df.groupby('movie_id', as_index=False, sort=False)[['token']].agg(lambda x: ' '.join(x)) # group tokens by movie\n",
        "    rows_list = []\n",
        "    for row in df.itertuples():\n",
        "        tokens_list = row.token.split(' ')\n",
        "        dict1 = {}\n",
        "        dict1.update({'movie_id': str(row.movie_id)})\n",
        "        dict1.update({'tokens': tokens_list})\n",
        "        rows_list.append(dict1)\n",
        "    df = pd.DataFrame(rows_list)\n",
        "    return df\n",
        "\n",
        "def save_tokens(df):\n",
        "    '''\n",
        "    Save the extracted tokens into files.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame with tokens, it must be the output of extract_tokens.\n",
        "    '''\n",
        "    \n",
        "    df = df.copy()\n",
        "    df['movie_id'] = df['movie_id'].astype(str)\n",
        "    \n",
        "    for row in df.itertuples():\n",
        "        tokens_path = os.path.join(row.movie_id, 'tokens')\n",
        "        if not os.path.exists(tokens_path):\n",
        "            os.makedirs(tokens_path)\n",
        "        tokens_path = os.path.join(tokens_path, 'extracted_tokens.txt')\n",
        "        with open(tokens_path, 'w', encoding='UTF8') as tokensFile:\n",
        "            tokens_list = row.tokens\n",
        "            for token in tokens_list:\n",
        "                tokensFile.write(token + '\\n')\n",
        "\n",
        "def extract_bigrams(df):\n",
        "    '''\n",
        "    Returns a DataFrame with filtered bigrams. Bigrams are filtered removing bigrams containing punctuation, comparative and superlative adjectives, \n",
        "    singular and plural proper nouns. Of the remaining bigrams only those matching this pos-tags are extracted: \n",
        "    adjective-noun, noun-noun, adjective-verb.\n",
        "    \n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame from which to extract bigrams, it must be the output of pos_tag.\n",
        "\n",
        "    Returns:\n",
        "        bigrams_df: The Dataframe with a bigram list associated to each movie.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    df['movie_id'] = df['movie_id'].astype(str)\n",
        "    df['review_id'] = df['review_id'].astype(str)\n",
        "    df['token'] = df['token'].astype(str)\n",
        "    df['upos'] = df['upos'].astype(str)\n",
        "    df['xpos'] = df['xpos'].astype(str)\n",
        "    df['feats'] = df['feats'].astype(str)\n",
        "    df['positivity'] = df['positivity'].astype(float)\n",
        "    df['negativity'] = df['negativity'].astype(float)\n",
        "    df['objectivity'] = df['objectivity'].astype(float)\n",
        "    \n",
        "    # delete unused columns to save memory\n",
        "    del df['feats']\n",
        "\n",
        "    # Filter bigrams using pos-tags\n",
        "    bigrams_df = pd.concat([df, df.shift(-1).add_prefix('next_')], axis=1)\n",
        "    bigrams_df.drop(bigrams_df[bigrams_df.review_id != bigrams_df.next_review_id].index, inplace=True) # delete rows with bigrams from different reviews\n",
        "    del bigrams_df['review_id'] #delete to save memory\n",
        "    del bigrams_df['next_review_id'] #delete to save memory\n",
        "    bigrams_df.drop(bigrams_df[((bigrams_df.upos == 'PUNCT') | (bigrams_df.next_upos == 'PUNCT'))].index, inplace=True) # delete rows with bigrams containing punctuation\n",
        "    bigrams_df.drop(bigrams_df[(bigrams_df.xpos == 'JJR') | (bigrams_df.next_xpos == 'JJR')].index, inplace=True) # delete rows with bigrams containing comparative adjectives\n",
        "    bigrams_df.drop(bigrams_df[(bigrams_df.xpos == 'JJS') | (bigrams_df.next_xpos == 'JJS')].index, inplace=True) # delete rows with bigrams containing superlative adjectives\n",
        "    bigrams_df.drop(bigrams_df[(bigrams_df.upos == 'PROPN') | (bigrams_df.next_upos == 'PROPN')].index, inplace=True) # delete rows with bigrams containing proper nouns\n",
        "    bigrams_df.drop(bigrams_df[(bigrams_df.token == '') | (bigrams_df.next_token == '')].index, inplace=True) # delete rows with bigrams where one token is empty\n",
        "    bigrams_df.drop(bigrams_df[~(((bigrams_df.xpos  == 'JJ') & (bigrams_df.next_upos  == 'NOUN')) | \\\n",
        "                                ((bigrams_df.upos  == 'NOUN') & (bigrams_df.next_upos  == 'NOUN')) \\\n",
        "                                )].index, inplace=True) #delete rows with bigrams not matching interesting couples\n",
        "    bigrams_df.drop(bigrams_df[(bigrams_df.positivity + bigrams_df.negativity + bigrams_df.objectivity == 0) | (bigrams_df.next_positivity + bigrams_df.next_negativity + bigrams_df.next_objectivity == 0)].index, inplace=True)\n",
        "\n",
        "    stopword_list = []\n",
        "    stopwords_folder = Path(\"stopwords/\")\n",
        "    file_path = stopwords_folder / \"bigrams_single_stopwords.txt\" #qua va messa la lista delle prime e ultime parole che decretano la morte di un bigramma\n",
        "    with open(file_path, 'r') as file:\n",
        "      lines = [line.rstrip('\\n') for line in file]\n",
        "    stopword_list.extend(lines)\n",
        "    for stopword in stopword_list:\n",
        "      bigrams_df.drop(bigrams_df[((bigrams_df.token == stopword) | (bigrams_df.next_token == stopword))].index, inplace=True)\n",
        "    \n",
        "    #Create bigrams\n",
        "    bigrams_df['bigrams'] = bigrams_df['token'] + ' ' + bigrams_df['next_token']\n",
        "\n",
        "    #Remove stopwords\n",
        "    bigrams_df = remove_stopwords(bigrams_df, 'bigram')\n",
        "\n",
        "    #Aggregate bigrams by movie\n",
        "    bigrams_df = bigrams_df.groupby('movie_id', as_index=False, sort=False)[['bigrams']].agg(lambda x: ','.join(x))\n",
        "\n",
        "    return bigrams_df\n",
        "\n",
        "def save_bigrams(df):\n",
        "    '''\n",
        "    Save the bigrams extracted into files.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame with bigrams, it must be extract_bigrams output.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    df['movie_id'] = df['movie_id'].astype(str)\n",
        "    \n",
        "    for row in df.itertuples():\n",
        "        bigrams_path = os.path.join(row.movie_id, 'bigrams')\n",
        "        if not os.path.exists(bigrams_path):\n",
        "            os.makedirs(bigrams_path)\n",
        "        bigrams_path = os.path.join(bigrams_path, 'extracted_bigrams.txt')\n",
        "        with open(bigrams_path, 'w', encoding='UTF8') as bigramsFile:\n",
        "            for bigram in row.bigrams.split(','):\n",
        "                bigramsFile.write(bigram + '\\n')\n",
        "\n",
        "def alphabetize_tokens(df):\n",
        "    '''\n",
        "    Returns the given dataframe after removing non alphabetic characters from all tokens.\n",
        "\n",
        "    Args:\n",
        "        df: The original dataframe.\n",
        "\n",
        "    Returns:\n",
        "        df: The DataFrame containing only alphabetic tokens.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    df['token'] = df['token'].astype(str)\n",
        "    df['token'] = df['token'].map(lambda x: alphabetize_string(x))\n",
        "    return df\n",
        "\n",
        "def alphabetize_string(string):\n",
        "    '''\n",
        "    Returns the given string after removing non alphabetic characters.\n",
        "\n",
        "    Args:\n",
        "        string: The original string.\n",
        "\n",
        "    Returns:\n",
        "        string: The string with non alphabetic characters removed.\n",
        "    '''\n",
        "\n",
        "    regex = r\"(\\w+-\\w+)|-+\"\n",
        "    string = re.sub(regex, r\"\\1\", string)\n",
        "    return string\n",
        "\n",
        "def remove_stopwords(df, mode):\n",
        "    '''\n",
        "    Returns the given df after removing stopwords.\n",
        "\n",
        "    Args:\n",
        "        df: The dataframe containing stopwords to remove.\n",
        "        mode: String that specifies if working with tokens or bigrams, it must be 'tokens' or 'bigrams'.\n",
        "\n",
        "    Returns:\n",
        "        df: The dataframe with stopwords removed.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    \n",
        "    stopword_list = []\n",
        "    stopwords_folder = Path(\"stopwords/\")\n",
        "\n",
        "    if mode == 'token':\n",
        "      file_path = stopwords_folder / \"ranks_nl_stopwords.txt\"\n",
        "      with open(file_path, 'r') as file:\n",
        "        lines = [line.rstrip('\\n') for line in file]\n",
        "      stopword_list.extend(lines)\n",
        "    \n",
        "      file_path = stopwords_folder / \"tokens_stopwords.txt\"\n",
        "      with open(file_path, 'r') as file:\n",
        "        lines = [line.rstrip('\\n') for line in file]\n",
        "      stopword_list.extend(lines)\n",
        "\n",
        "      file_path = stopwords_folder / \"wikidata_tok_stopwords.txt\"\n",
        "      with open(file_path, 'r') as file:\n",
        "        lines = [line.rstrip('\\n') for line in file]\n",
        "      stopword_list.extend(lines)\n",
        "\n",
        "      df['token'] = df['token'].astype(str)\n",
        "      df['stop'] = df['token'].map(lambda x: is_stopword(x, stopword_list))\n",
        "    else:\n",
        "      file_path = stopwords_folder / \"bigrams_stopwords.txt\"\n",
        "      with open(file_path, 'r') as file:\n",
        "        lines = [line.rstrip('\\n') for line in file]\n",
        "      stopword_list.extend(lines)\n",
        "\n",
        "      file_path = stopwords_folder / \"wikidata_big_stopwords.txt\"\n",
        "      with open(file_path, 'r') as file:\n",
        "        lines = [line.rstrip('\\n') for line in file]\n",
        "      stopword_list.extend(lines)\n",
        "\n",
        "      df['bigrams'] = df['bigrams'].astype(str)\n",
        "      df['stop'] = df['bigrams'].map(lambda x: is_stopword(x, stopword_list))\n",
        "\n",
        "    df.drop(df[(df.stop)].index, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    del df['stop']\n",
        "    return df\n",
        "\n",
        "def is_stopword(string, stopword_list):\n",
        "    '''\n",
        "    Returns True if the given string is a stopword, false otherwise.\n",
        "\n",
        "    Args:\n",
        "        string: The string to check against stopword lists.\n",
        "    '''\n",
        "\n",
        "    if string in stopword_list:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def join_dataframes(start, stop, aspect_type, what):\n",
        "  '''\n",
        "  Joins the splitted results of aspect extraction.\n",
        "\n",
        "  Args:\n",
        "      start: the first piece to join\n",
        "      stop: the last piece to join\n",
        "      aspect_type:  what kind of aspect to join, it can be 'tokens' or 'bigrams'\n",
        "      what: what to join, it can be 'all_tokens', 'all_bigrams', 'tokens_term_freq' or 'bigrams_term_freq'\n",
        "  '''\n",
        "\n",
        "  path = os.path.join('all_movies', str(aspect_type))\n",
        "  df0 = pd.read_csv(os.path.join(path, str(what) + '_' + str(start) + '.txt'), sep='|')\n",
        "  for i in range(start + 1, stop + 1):\n",
        "    df = pd.read_csv(os.path.join(path, str(what) + '_' + str(i) + '.txt'), sep='|')\n",
        "    df0 = df0.append(df, ignore_index=True)\n",
        "    del df\n",
        "    gc.collect()\n",
        "\n",
        "  df0.to_csv(str(what) + '_dataset.txt', index=None, sep='|')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL29Uo5LnFel"
      },
      "source": [
        "## Aspect Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2BpcOcTf6oo"
      },
      "source": [
        "from ast import literal_eval\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "from time import strftime, gmtime\n",
        "from pathlib import Path\n",
        "\n",
        "def term_freq(df, mode):\n",
        "    '''\n",
        "    Returns a Dataframe containing the term frequency for each token/bigram aggregated by movie.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing tokens or bigrams, it must be the output of extract_tokens or extract_bigrams.\n",
        "        mode: String that specifies if working with tokens or bigrams, it must be 'tokens' or 'bigrams'.\n",
        "\n",
        "    Returns:\n",
        "        term_freq_concat: The Dataframe with the term frequency for each token/bigram.\n",
        "        \n",
        "    Raises:\n",
        "        Exception: If mode not specified.\n",
        "    '''\n",
        "    \n",
        "    if not mode:\n",
        "         raise Exception(\"You must set a mode, available modes are 'tokens' and 'bigrams'\") \n",
        "    df = df.copy()\n",
        "    df['movie_id'] = df['movie_id'].astype(str)\n",
        "    \n",
        "    term_freq_concat = pd.DataFrame()\n",
        "    for row in df.itertuples():\n",
        "        if mode == 'tokens':\n",
        "            wordlist = literal_eval(str(row.tokens))\n",
        "            file_suffix = '_tokens_term_freq.txt'\n",
        "        else:\n",
        "            wordlist = row.bigrams.split(',')\n",
        "            file_suffix = '_bigrams_term_freq.txt'\n",
        "        dictionary = wordListToFreqDict(wordlist)\n",
        "        sorted_dict = sortFreqDict(dictionary)\n",
        "        term_freq_df = pd.DataFrame(sorted_dict, columns=['term_freq', 'term'])\n",
        "        term_freq_df['movie_id'] = str(row.movie_id)\n",
        "        term_freq_concat = pd.concat([term_freq_concat, term_freq_df])\n",
        "    term_freq_concat.sort_values(by='movie_id', inplace=True, ascending=False)\n",
        "    term_freq_concat.reset_index(drop=True, inplace=True)\n",
        "    return term_freq_concat\n",
        "\n",
        "def wordListToFreqDict(wordlist):\n",
        "    '''\n",
        "    Returns a frequency dictionary for the wordlist.\n",
        "\n",
        "    Args:\n",
        "        wordlist: The list of terms.\n",
        "\n",
        "    Returns:\n",
        "        dict1: The frequency dictionary.\n",
        "    '''\n",
        "    \n",
        "    doc_dim = len(wordlist)\n",
        "    wordfreq = [wordlist.count(p)/doc_dim for p in wordlist]\n",
        "    dict1 = dict(list(zip(wordlist,wordfreq)))\n",
        "    return dict1\n",
        "\n",
        "def sortFreqDict(freqdict):\n",
        "    '''\n",
        "    Returns the sorted frequency dictionary.\n",
        "\n",
        "    Args:\n",
        "        freqdict: The frequence dictionary to sort.\n",
        "\n",
        "    Returns:\n",
        "        aux: The sorted frequency dictionary.\n",
        "    '''\n",
        "    \n",
        "    aux = [(freqdict[key], key) for key in freqdict]\n",
        "    aux.sort()\n",
        "    aux.reverse()\n",
        "    return aux\n",
        "\n",
        "def doc_freq(df, mode):\n",
        "    '''\n",
        "    Returns a Dataframe containing the document frequency for each token/bigram.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing tokens or bigrams, it must be the output of extract_tokens or extract_bigrams.\n",
        "        mode: String that specifies if working with tokens or bigrams, it must be 'tokens' or 'bigrams'.\n",
        "\n",
        "    Returns:\n",
        "        doc_freq_df: The Dataframe with the document frequency for each token/bigram.\n",
        "        \n",
        "    Raises:\n",
        "        Exception: If mode not specified.\n",
        "    '''\n",
        "    \n",
        "    if not mode:\n",
        "         raise Exception(\"You must set a mode, available modes are 'tokens' and 'bigrams'\") \n",
        "    df = df.copy()\n",
        "    df['movie_id'] = df['movie_id'].astype(str)\n",
        "    \n",
        "    doc_freq_dict = {}\n",
        "    for row in df.itertuples():\n",
        "        if mode == 'tokens':\n",
        "            df['tokens'] = df['tokens'].astype(str)\n",
        "            items = literal_eval(str(row.tokens))\n",
        "        else:\n",
        "            items = row.bigrams.split(',')\n",
        "        for w in items:\n",
        "            try:\n",
        "                doc_freq_dict[w].add(row.movie_id)\n",
        "            except:\n",
        "                doc_freq_dict[w] = {row.movie_id}\n",
        "    for i in doc_freq_dict:\n",
        "        doc_freq_dict[i] = len(doc_freq_dict[i])\n",
        "    doc_freq_df = pd.DataFrame.from_dict(doc_freq_dict, orient='index').reset_index()\n",
        "    doc_freq_df.columns = np.arange(len(doc_freq_df.columns))\n",
        "    doc_freq_df.columns = ['term', 'doc_freq']\n",
        "    doc_freq_df.sort_values(by='doc_freq', inplace=True, ascending=False)\n",
        "    doc_freq_df = doc_freq_df.reset_index()\n",
        "    del doc_freq_df['index']\n",
        "    return doc_freq_df\n",
        "\n",
        "def tf_idf(term_freq, doc_freq, doc_number):\n",
        "    '''\n",
        "    Returns a Dataframe containing the tf-idf for each token/bigram.\n",
        "\n",
        "    Args:\n",
        "        term_freq: The DataFrame containing term frequency, it must be the output of term_freq.\n",
        "        doc_freq: The DataFrame containing document frequency, it must be the output of doc_freq.\n",
        "        doc_number: The number of documents for idf calculation, it is equal to the number of movies analyzed\n",
        "\n",
        "    Returns:\n",
        "        tfidf_df: The Dataframe with the tf-idf score for each token/bigram.\n",
        "    '''\n",
        "    \n",
        "    term_freq = term_freq.copy()\n",
        "    term_freq['term_freq'] = term_freq['term_freq'].astype(float)\n",
        "    term_freq['term'] = term_freq['term'].astype(str)\n",
        "    \n",
        "    doc_freq = doc_freq.copy()\n",
        "    doc_freq['doc_freq'] = doc_freq['doc_freq'].astype(int)\n",
        "    doc_freq['term'] = doc_freq['term'].astype(str)\n",
        "    \n",
        "    tfidf_df = term_freq.merge(doc_freq, on=\"term\")\n",
        "    tfidf_df['tf_idf'] = tfidf_df['term_freq'] * np.log10(doc_number/tfidf_df['doc_freq'])\n",
        "    del tfidf_df['term_freq']\n",
        "    del tfidf_df['doc_freq']\n",
        "    tfidf_df.sort_values(by='tf_idf', inplace=True, ascending=False)\n",
        "    return tfidf_df\n",
        "\n",
        "def df_tfidf(movie_id_list, tokens_df, bigrams_df, tokens_term_freq, bigrams_term_freq, movies_number):\n",
        "  '''\n",
        "  Calculates the tf-idf.\n",
        "\n",
        "  Args:\n",
        "      movie_id_list: list of movie ids to consider during tf-idf calculation\n",
        "      tokens_df: dataframe with all the extracted tokens, it can be obtained by joining together all partial dataframes\n",
        "      bigrams_df: dataframe with all the extracted bigrams, it can be obtained by joining together all partial dataframes\n",
        "      tokens_term_freq: dataframe with the tokens term frequency, it can be obtained by joining together all partial dataframes\n",
        "      bigrams_term_freq: dataframe with the bigrams term frequency, it can be obtained by joining together all partial dataframes\n",
        "      movies_number: number of movies in movie_id_list \n",
        "\n",
        "  '''\n",
        "  \n",
        "  # Define tokens path\n",
        "  tokens_path = os.path.join('all_movies', 'tokens')\n",
        "  if not os.path.exists(tokens_path):\n",
        "    os.makedirs(tokens_path)\n",
        "  \n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  print(time + ' tokens document frequency... \\n')\n",
        "\n",
        "  # Tokens document frequency\n",
        "  tokens_doc_freq = doc_freq(tokens_df, 'tokens')\n",
        "  path = os.path.join(tokens_path, 'tokens_doc_freq.txt')\n",
        "  tokens_doc_freq.to_csv(path, sep=\"|\", header=['term', 'doc_freq'], index=None)\n",
        "    \n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  print(time + ' tokens tf-idf... \\n')    \n",
        "\n",
        "  # Tokens tf-idf for each movie\n",
        "  tokens_tfidf = tf_idf(tokens_term_freq, tokens_doc_freq, movies_number) # calculate tokens tfidf\n",
        "  path = os.path.join(tokens_path, 'tokens_tfidf.txt')\n",
        "  tokens_tfidf.to_csv(path, sep=\"|\", header=['term', 'movie_id', 'tf_idf'], index=None)\n",
        "  \n",
        "  per_movie_freq_list(movie_id_list, tokens_doc_freq, 'tokens', 'doc_freq')\n",
        "  per_movie_freq_list(movie_id_list, tokens_tfidf, 'tokens', 'tf_idf')\n",
        "  \n",
        "  # Define bigrams path\n",
        "  bigrams_path = os.path.join('all_movies', 'bigrams')\n",
        "  if not os.path.exists(bigrams_path):\n",
        "    os.makedirs(bigrams_path)\n",
        "\n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  print(time + ' bigrams document frequency... \\n')\n",
        "\n",
        "  # Bigrams document frequency\n",
        "  bigrams_doc_freq = doc_freq(bigrams_df, 'bigrams')\n",
        "  path = os.path.join(bigrams_path, 'bigrams_doc_freq.txt')\n",
        "  bigrams_doc_freq.to_csv(path, sep=\"|\", header=['term', 'doc_freq'], index=None)\n",
        "\n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  print(time + ' bigrams tf-idf... \\n')  \n",
        "    \n",
        "  # Bigrams tf-idf for each movie\n",
        "  bigrams_tfidf = tf_idf(bigrams_term_freq, bigrams_doc_freq, movies_number)\n",
        "  path = os.path.join(bigrams_path, 'bigrams_tfidf.txt')\n",
        "  bigrams_tfidf.to_csv(path, sep=\"|\", header=['term', 'movie_id', 'tf_idf'], index=None)\n",
        "\n",
        "  per_movie_freq_list(movie_id_list, bigrams_doc_freq, 'bigrams', 'doc_freq')\n",
        "  per_movie_freq_list(movie_id_list, bigrams_tfidf, 'bigrams', 'tf_idf')\n",
        "  \n",
        "  return\n",
        "\n",
        "def per_movie_freq_list(movie_id_list, df, mode, df_type):\n",
        "  '''\n",
        "  Create files with term frequency, document frequency or tf-idf for tokens or bigrams\n",
        "\n",
        "  Args:\n",
        "      movie_id_list: list of movie ids to consider creating the list\n",
        "      df: the dataframe with term frequency, document frequency or tf-idf\n",
        "      mode: what kind of aspects are in df, it can be 'tokens' or 'bigrams'\n",
        "      df_type: what kind of dataframe is df, it can be 'term_freq', 'doc_freq' or 'tf-idf'\n",
        "  '''\n",
        "\n",
        "  if not df_type:\n",
        "    raise Exception(\"You must set a df_type, available df_types are 'term_freq', 'doc_freq' and 'tf_idf\") \n",
        "\n",
        "  if not mode:\n",
        "    raise Exception(\"You must set a mode, available modes are 'tokens' and 'bigrams'\") \n",
        "\n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  print(time + ' per movie ' + str(mode) + str(df_type) + '\\n')  \n",
        "\n",
        "  df = df.copy()\n",
        "  \n",
        "  i = 0\n",
        "  for movie_id in movie_id_list:\n",
        "    i+=1\n",
        "    path = os.path.join(movie_id, mode)\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "    if df_type == 'doc_freq':\n",
        "      try:\n",
        "        extracted_aspect_terms = pd.read_csv(os.path.join(path, 'extracted_' + str(mode) + '.txt'), sep='\\n', names=['term'])\n",
        "      except OSError as e:\n",
        "        break\n",
        "      movie_df = df.merge(extracted_aspect_terms, on = 'term')\n",
        "      movie_df.drop_duplicates(inplace=True)\n",
        "      movie_df['movie_id'] = movie_id\n",
        "    else:\n",
        "      movie_df = df.loc[df['movie_id'] == movie_id]\n",
        "\n",
        "    movie_df.sort_values(by=df_type, ascending=False, ignore_index=True, inplace=True)\n",
        "    del movie_df['movie_id']\n",
        "    if df_type != 'term_freq':\n",
        "      movie_df.to_csv(os.path.join(path, str(mode) + '_' + str(df_type) + '.txt'), sep='|', header=['term', str(df_type)], index=None)\n",
        "    else:\n",
        "      movie_df.to_csv(os.path.join(path, str(mode) + '_' + str(df_type) + '.txt'), sep='|', header=[str(df_type), 'term'], index=None)\n",
        "    del movie_df\n",
        "    gc.collect()\n",
        "    \n",
        "  return\n",
        "\n",
        "def top_k_and_overlapping(movie_id_list, k):\n",
        "  '''Extract the top k for tokens and bigrams coming from movies in movie_id_list'''\n",
        "  extract_movie_tok_top_k(movie_id_list, k) # crea la top k per i token\n",
        "  extract_movie_big_top_k(movie_id_list, k)# crea la top k per i bigrammi\n",
        "\n",
        "  good_bigrams_to_tok(movie_id_list, k)\n",
        "\n",
        "  tok_over_df = tokens_overlapping_list(movie_id_list, k) #calcola gli aspetti in overlapping per i token\n",
        "  big_over_df = bigrams_overlapping_list(movie_id_list, k) #calcola gli aspetti in overlapping per i bigrammi\n",
        "  tok_over_df.to_csv('tokens_overlapping_list_' + str(k) + '.txt', sep='|', index=None)\n",
        "  big_over_df.to_csv('bigrams_overlapping_list_' + str(k) + '.txt', sep='|', index=None)\n",
        "\n",
        "  return\n",
        "\n",
        "def extract_movie_tok_top_k(id_list, k):\n",
        "  '''Extract the top k list for tokens by tf-idf for each movie in id_list'''\n",
        "\n",
        "  for movie_id in id_list:\n",
        "    tokens_path = os.path.join(movie_id,'tokens')\n",
        "    tokens_tfidf = pd.read_csv(os.path.join(tokens_path, 'tokens_tf_idf.txt'), sep='|')\n",
        "    tokens_top = tokens_tfidf.head(k)\n",
        "    tokens_top.to_csv(os.path.join(tokens_path, 'top' + str(k) + '_tokens.txt'), sep='|', index=None)\n",
        "\n",
        "def extract_movie_big_top_k(id_list, k):\n",
        "  '''Extract the top k list for bigrams by tf-idf for each movie in id_list'''\n",
        "\n",
        "  for movie_id in id_list:\n",
        "    bigrams_path = os.path.join(movie_id, 'bigrams')\n",
        "    bigrams_tfidf = pd.read_csv(os.path.join(bigrams_path, 'bigrams_tf_idf.txt'), sep='|')\n",
        "    bigrams_top = bigrams_tfidf.head(k)\n",
        "    bigrams_top.to_csv(os.path.join(bigrams_path, 'top' + str(k) + '_bigrams.txt'), sep='|', index=None)\n",
        "\n",
        "def aspect_term_tok_list(id_list, k):\n",
        "  '''Concatenates together the top k tokens list for every movie in id_list'''\n",
        "\n",
        "  concat_df = pd.DataFrame()\n",
        "\n",
        "  for movie_id in id_list:\n",
        "    tokens_path = os.path.join(movie_id,'tokens')\n",
        "    df = pd.read_csv(os.path.join(tokens_path, 'top' + str(k) + '_tokens.txt'), sep='|')\n",
        "    del df['tf_idf']\n",
        "    \n",
        "    concat_df = pd.concat([concat_df, df])\n",
        "  \n",
        "  concat_df['term'] = concat_df['term'].astype(str)\n",
        "\n",
        "  concat_df.drop_duplicates(inplace=True)\n",
        "  \n",
        "  concat_df.reset_index(inplace=True, drop=True)\n",
        "  concat_df['term_id'] = concat_df.index\n",
        "\n",
        "  cols = list(concat_df.columns)\n",
        "  a, b = cols.index('term'), cols.index('term_id')\n",
        "  cols[b], cols[a] = cols[a], cols[b]\n",
        "  concat_df = concat_df[cols]\n",
        "\n",
        "  return concat_df\n",
        "\n",
        "def aspect_term_big_list(id_list, k):\n",
        "  '''Concatenates together the top k bigrams list for every movie in id_list'''\n",
        "\n",
        "  concat_df = pd.DataFrame()\n",
        "\n",
        "  for movie_id in id_list:\n",
        "    bigrams_path = os.path.join(movie_id,'bigrams')\n",
        "    df = pd.read_csv(os.path.join(bigrams_path, 'top' + str(k) +'_bigrams.txt'), sep='|')\n",
        "    del df['tf_idf']\n",
        "\n",
        "    concat_df = pd.concat([concat_df, df])\n",
        "\n",
        "  concat_df['term'] = concat_df['term'].astype(str)\n",
        "\n",
        "  concat_df.drop_duplicates(inplace=True)\n",
        "  \n",
        "  concat_df.reset_index(inplace=True, drop=True)\n",
        "  \n",
        "  concat_df.index += 100000\n",
        "  concat_df['term_id'] = concat_df.index\n",
        "\n",
        "  cols = list(concat_df.columns)\n",
        "  a, b = cols.index('term'), cols.index('term_id')\n",
        "  cols[b], cols[a] = cols[a], cols[b]\n",
        "  concat_df = concat_df[cols]\n",
        "\n",
        "  return concat_df\n",
        "\n",
        "def map_aspect_terms(movie_id_list, k, map_tokens=True, map_bigrams=True):\n",
        "  '''Extract k aspect terms per movie and create the mapping files needed for Converse'''\n",
        "\n",
        "  if map_tokens:\n",
        "    time = strftime(\"%H:%M:%S\", gmtime())\n",
        "    print(time + ' mapping tokens... \\n')\n",
        "    df = aspect_term_tok_list(movie_id_list, k) #concatenates together the top k for every movie in movie_id_list\n",
        "    tok_list_clean_df = pd.read_csv('tokens_overlapping_list_clean.txt', sep='|')\n",
        "    df = df.merge(tok_list_clean_df, on='term')\n",
        "    del df['occurrences']\n",
        "    df.to_csv('tokens_list.txt', sep='|', index=None, header=None)\n",
        "    df = tokens_mapping(movie_id_list, k)\n",
        "    df.to_csv('tokens_mapping.txt', sep='|', index=None, header=None)\n",
        "\n",
        "  if map_bigrams:\n",
        "    time = strftime(\"%H:%M:%S\", gmtime())\n",
        "    print(time + ' mapping bigrams... \\n')\n",
        "    df = aspect_term_big_list(movie_id_list, k) #concatenates together the top k for every movie in movie_id_list\n",
        "    big_list_clean_df = pd.read_csv('bigrams_overlapping_list_clean.txt', sep='|')\n",
        "    df = df.merge(big_list_clean_df, on='term')\n",
        "    del df['occurrences']\n",
        "    df.to_csv('bigrams_list.txt', sep='|', index=None, header=None)\n",
        "    df = bigrams_mapping(movie_id_list, k)\n",
        "    df.to_csv('bigrams_mapping.txt', sep='|', index=None, header=None)\n",
        "\n",
        "def tokens_mapping(id_list, k):\n",
        "  '''maps the extracted k tokens to the movies in id_list they come from'''\n",
        "\n",
        "  aspect_term_df = pd.read_csv('tokens_list.txt', sep='|', names = ['term_id', 'term'])\n",
        "  aspect_term_df['term_id'] = aspect_term_df['term_id'].astype(int)\n",
        "  concat_df = pd.DataFrame()\n",
        "\n",
        "  for movie_id in id_list:\n",
        "    movie_tokens_path = os.path.join(movie_id,'tokens')\n",
        "\n",
        "    df = pd.read_csv(os.path.join(movie_tokens_path, 'top' + str(k) + '_tokens.txt'), sep='|') #df with all tokens extracted from the movie\n",
        "    del df['tf_idf']\n",
        "    \n",
        "    df = df.merge(aspect_term_df, on='term')\n",
        "    \n",
        "    df['movie_id'] = movie_id\n",
        "    del df['term']\n",
        "\n",
        "    concat_df = pd.concat([concat_df, df])\n",
        "  \n",
        "  concat_df['property_type'] = 'review'\n",
        "\n",
        "  cols = list(concat_df.columns)\n",
        "  a, b, c = cols.index('term_id'), cols.index('movie_id'), cols.index('property_type')\n",
        "  cols[b], cols[a], cols[c] = cols[c], cols[b], cols[a]\n",
        "  concat_df = concat_df[cols]\n",
        "\n",
        "  return concat_df\n",
        "\n",
        "def bigrams_mapping(id_list, k):\n",
        "  '''maps the extracted k bigrams to the movies in id_list they come from'''\n",
        "\n",
        "  aspect_term_df = pd.read_csv('bigrams_list.txt', sep='|', names = ['term_id', 'term'])\n",
        "  aspect_term_df['term_id'] = aspect_term_df['term_id'].astype(int)\n",
        "  concat_df = pd.DataFrame()\n",
        "\n",
        "  for movie_id in id_list:\n",
        "    movie_bigrams_path = os.path.join(movie_id,'bigrams')\n",
        "\n",
        "    df = pd.read_csv(os.path.join(movie_bigrams_path, 'top' + str(k) +'_bigrams.txt'), sep='|') #df with all tokens extracted from the movie\n",
        "    del df['tf_idf']\n",
        "    \n",
        "    df = df.merge(aspect_term_df, on='term')\n",
        "    \n",
        "    df['movie_id'] = movie_id\n",
        "    del df['term']\n",
        "\n",
        "    concat_df = pd.concat([concat_df, df])\n",
        "  \n",
        "  concat_df['property_type'] = 'review'\n",
        "\n",
        "  cols = list(concat_df.columns)\n",
        "  a, b, c = cols.index('term_id'), cols.index('movie_id'), cols.index('property_type')\n",
        "  cols[b], cols[a], cols[c] = cols[c], cols[b], cols[a]\n",
        "  concat_df = concat_df[cols]\n",
        "\n",
        "  return concat_df\n",
        "\n",
        "def tokens_overlapping_list(id_list, k):\n",
        "  '''calculates the number of movies in which each token appears'''\n",
        "\n",
        "  concat_df = pd.DataFrame()\n",
        "\n",
        "  for movie_id in id_list:\n",
        "    tokens_path = os.path.join(movie_id,'tokens')\n",
        "    df = pd.read_csv(os.path.join(tokens_path, 'top' + str(k) + '_tokens.txt'), sep='|')\n",
        "    del df['tf_idf']\n",
        "    \n",
        "    concat_df = pd.concat([concat_df, df])\n",
        "  \n",
        "  concat_df['term'] = concat_df['term'].astype(str)\n",
        "\n",
        "  concat_df.reset_index()\n",
        "\n",
        "  concat_df = concat_df.groupby(['term']).size().reset_index(name='occurrences')\n",
        "  concat_df.sort_values(by='occurrences', inplace=True, ascending=False)\n",
        "\n",
        "  return concat_df\n",
        "\n",
        "def bigrams_overlapping_list(id_list, k):\n",
        "  '''calculates the number of movies in which each bigram appears'''\n",
        "\n",
        "  concat_df = pd.DataFrame()\n",
        "\n",
        "  for movie_id in id_list:\n",
        "    bigrams_path = os.path.join(movie_id,'bigrams')\n",
        "    df = pd.read_csv(os.path.join(bigrams_path, 'top' + str(k) +'_bigrams.txt'), sep='|')\n",
        "    del df['tf_idf']\n",
        "\n",
        "    concat_df = pd.concat([concat_df, df])\n",
        "\n",
        "  concat_df['term'] = concat_df['term'].astype(str)\n",
        "\n",
        "  concat_df.reset_index()\n",
        "\n",
        "  concat_df = concat_df.groupby(['term']).size().reset_index(name='occurrences')\n",
        "  concat_df.sort_values(by='occurrences', inplace=True, ascending=False)\n",
        "\n",
        "  return concat_df\n",
        "\n",
        "def count_aspect_terms_frequency(movie_id_list, k):\n",
        "  '''calls tokens_overlapping_list and bigrams_overlapping_list and saves results to files'''\n",
        "\n",
        "  tok_over_df = tokens_overlapping_list(movie_id_list, k)\n",
        "  big_over_df = bigrams_overlapping_list(movie_id_list, k)\n",
        "  tok_over_df.to_csv(\"tok_overlap.txt\", sep='|', index=None)\n",
        "  big_over_df.to_csv(\"big_overlap.txt\", sep='|', index=None)\n",
        "\n",
        "def remove_good(token):\n",
        "  '''remove the adjective from positive bigrams'''\n",
        "  index = token.find(' ') + 1\n",
        "  return token[index:]\n",
        "\n",
        "def good_bigrams_to_tok(movie_id_list, k):\n",
        "\n",
        "  search = 'beautiful decent enjoyable entertaining excellent good great nice worthy superb wonderful amazing' # list of adjectives to eliminate from bigrams\n",
        "\n",
        "  for movie_id in movie_id_list:\n",
        "    bigrams_path = os.path.join(movie_id, 'bigrams', 'top' + str(k) + '_bigrams.txt') # define bigrams path\n",
        "    tokens_path = os.path.join(movie_id, 'tokens', 'top' + str(k) + '_tokens.txt') # define tokens path\n",
        "\n",
        "    bigrams_df = pd.read_csv(bigrams_path, sep = '|') # read bigrams file\n",
        "\n",
        "    res = bigrams_df[[any(i in words for i in search.split()) for words in bigrams_df['term'].str.split().values]] # get good/great bigrams\n",
        "    if not res.empty:\n",
        "      res['term'] = res['term'].map(lambda x: remove_good(x)) # remove good/great token from those bigrams\n",
        "      res.rename(columns = {'term':'token'}, inplace = True)\n",
        "      res = remove_stopwords(res, 'token')\n",
        "      res.rename(columns = {'token':'term'}, inplace = True)\n",
        "      res.drop_duplicates(subset='term', inplace=True)\n",
        "\n",
        "      bigrams_df.drop(bigrams_df[[any(i in words for i in search.split()) for words in bigrams_df['term'].str.split().values]].index, inplace=True) # drop the good/great bigrams form bigrams file\n",
        "      bigrams_df.reset_index()\n",
        "      bigrams_df.to_csv(bigrams_path, sep = '|', index = None)\n",
        "\n",
        "      tokens_df = pd.read_csv(tokens_path, sep = '|') \n",
        "      tokens_list = res['term']\n",
        "      tokens_df['term'] = tokens_df['term'].astype(str)\n",
        "      tokens_df.drop(tokens_df[[any(i in words for i in tokens_list) for words in tokens_df['term'].str.split().values]].index, inplace=True)\n",
        "  \n",
        "      concat_df = pd.concat([tokens_df, res])\n",
        "      concat_df.reset_index()\n",
        "      concat_df.to_csv(tokens_path, sep = '|', index = None)\n",
        "  \n",
        "def edit_aspects_label(df, mode):\n",
        "    '''\n",
        "    Returns the given df after modifying the label of the aspects that have the same name as a movie.\n",
        "\n",
        "    Args:\n",
        "        df: The dataframe containing stopwords to remove.\n",
        "        mode: String that specifies if working with tokens or bigrams, it must be 'token' or 'bigram'.\n",
        "    '''\n",
        "\n",
        "    df = df.copy()\n",
        "    \n",
        "    movie_names_list = []\n",
        "    stopwords_folder = Path(\"stopwords/\")\n",
        "\n",
        "    if mode == 'token':\n",
        "      file_path = stopwords_folder / \"tokens_movie_names_stopwords.txt\"\n",
        "      with open(file_path, 'r') as file:\n",
        "        lines = [line.rstrip('\\n') for line in file]\n",
        "      movie_names_list.extend(lines)\n",
        "\n",
        "    else:\n",
        "      file_path = stopwords_folder / \"bigrams_movie_names_stopwords.txt\"\n",
        "      with open(file_path, 'r') as file:\n",
        "        lines = [line.rstrip('\\n') for line in file]\n",
        "      movie_names_list.extend(lines)\n",
        "\n",
        "      file_path = stopwords_folder / \"wikidata_big_stopwords.txt\"\n",
        "      with open(file_path, 'r') as file:\n",
        "        lines = [line.rstrip('\\n') for line in file]\n",
        "      movie_names_list.extend(lines)\n",
        "\n",
        "    df['term'] = df['term'].astype(str)\n",
        "\n",
        "    df['term'] = df['term'].map(lambda x: edit_label(x, movie_names_list))\n",
        "\n",
        "    return df\n",
        "\n",
        "def edit_label(string, movie_names_list):\n",
        "    '''\n",
        "    Returns True if the given string is a stopword, false otherwise.\n",
        "\n",
        "    Args:\n",
        "        string: The string to check against stopword lists.\n",
        "        movie_names_list: list of movie names that trigger the editing of the label\n",
        "    '''\n",
        "    if string in movie_names_list:\n",
        "        return string + ' (property)'\n",
        "    else:\n",
        "        return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSFP2DTG7oit"
      },
      "source": [
        "# Esecuzione\n",
        "Eseguendo le celle di seguito è possibile estrarre gli aspetti a partire dai vari pezzi di dataset. \n",
        "È possibile definire quali pezzi di dataset si vogliono utilizzare modificando i parametri start e stop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD8i_LZkaaEI"
      },
      "source": [
        "start = 0 # primo pezzo di dataset da elaborare, default = 0\n",
        "stop = 39 # ultimo pezzo di dataset da elaborare, default = 39"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmNbILQeEy1y"
      },
      "source": [
        "## Pre-processing\n",
        "Esegue le funzioni di Pre-processing sui vari pezzi di dataset salvando i risultati in altrettanti pezzi. \n",
        "*   I file in splitted_sentiment sono i token risultanti da ogni frase associati al sentiment rilevato per la frase di provenienza;\n",
        "*   I file in splitted_processed_tokens sono i token provenienti dalle frasi con sentiment positivo associati ai relativi PoS tag e sentiment score calcolato da SentiWordNet.\n",
        "\n",
        "Input di questa fase: pezzi di dataset contenente le recensioni (file in splitted_dataset);\n",
        "\n",
        "Output di questa fase: pezzi di dataset elaborati (file in splitted_sentiment e splitted_processed_tokens)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9ELG8vzDYdQ"
      },
      "source": [
        "sentiment_path = 'splitted_sentiment'\n",
        "processed_tokens_path = 'splitted_processed_tokens'\n",
        "\n",
        "for counter in range (start, stop + 1):\n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  print(time + ' Pre-processing df' + str(counter) + '\\n')\n",
        "  sample_df = pd.read_csv(os.path.join('splitted_dataset', 'df' + str(counter) + '.txt'), sep='|', header=None, names=['movie_id', 'review_id', 'review_text'])\n",
        "\n",
        "  sentiment_df = sentiment_analysis(sample_df)\n",
        "  sentiment_df.to_csv(os.path.join(sentiment_path, 'sentiment' + str(counter) + '.txt'), sep=\"|\", index=None, header=['movie_id', 'review_id', 'token', 'sentiment'])\n",
        "  sentiment_df = pd.read_csv(os.path.join(sentiment_path, 'sentiment' + str(counter) + '.txt'), sep='|', header=None, names=['movie_id', 'review_id', 'token', 'sentiment'])\n",
        "\n",
        "  pass1 = filter_by_sentiment(sentiment_df, 'positive')\n",
        "  pass2 = pos_lemma(pass1)\n",
        "  pass3 = lowercase(pass2)\n",
        "  processed_tokens_df = calculate_swn_score(pass3)\n",
        "\n",
        "  processed_tokens_df.to_csv(os.path.join(processed_tokens_path, 'processed_tokens' + str(counter) + '.txt'), sep='|', index=None, header=['movie_id', 'review_id', 'token', 'upos', 'xpos', 'feats', 'positivity', 'negativity', 'objectivity'])\n",
        "  \n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  print(time + ' Pre-processing df' + str(counter) + ' DONE \\n')\n",
        "\n",
        "time = strftime(\"%H:%M:%S\", gmtime())\n",
        "print(time + ' Pre-processing COMPLETE \\n')\n",
        "print('===========================\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzf-rYHF6PIi"
      },
      "source": [
        "## Aspect Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8B3SrOHlRXf"
      },
      "source": [
        "Esegue le funzioni di aspect extraction partendo dai file in splitted_processed_tokens. Da questa estrazione si ottengono unigrammi, bigrammi e relativa term frequency.\n",
        "\n",
        "Input di questa fase: pezzi di dataset in splitted_processed_tokens;\n",
        "\n",
        "Output di questa fase: liste di unigrammi e bigrammi estratti per ogni film e relativa term frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caZoLzb95sTr"
      },
      "source": [
        "from time import strftime, gmtime\n",
        "from pathlib import Path\n",
        "\n",
        "for counter in range (start, stop + 1):\n",
        "  tokens_extraction = True\n",
        "  bigrams_extraction = True\n",
        "\n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  splitted_df_folder = Path(\"splitted_processed_tokens/\")\n",
        "  processed_tokens_df_str = ('processed_tokens' + str(counter) + '.txt')\n",
        "  file_path = splitted_df_folder / processed_tokens_df_str\n",
        "  print(time + ' Extracting from processed_tokens_df: ' + processed_tokens_df_str + '\\n')\n",
        "  processed_tokens_df = pd.read_csv(file_path, sep='|')\n",
        "  movies_number = len(processed_tokens_df['movie_id'].value_counts().index) # calculate number of movies in the dataframe, used for document frequency calculation\n",
        "  time = strftime(\"%H:%M:%S\", gmtime())\n",
        "  print(time + ' movies_number: ' + str(movies_number) + '\\n')\n",
        "  movie_id_list = processed_tokens_df['movie_id'].unique() # create a list of all the movies id in the considered piece of dataset\n",
        "\n",
        "  # Tokens extraction\n",
        "  if tokens_extraction:\n",
        "    time = strftime(\"%H:%M:%S\", gmtime())\n",
        "    print(time + ' extracting tokens... \\n')\n",
        "    # Define tokens path\n",
        "    tokens_path = os.path.join('all_movies', 'tokens')\n",
        "    if not os.path.exists(tokens_path):\n",
        "        os.makedirs(tokens_path)\n",
        "    # tokens extraction\n",
        "    tokens_df = extract_tokens(processed_tokens_df) # filter and extract tokens\n",
        "    path = os.path.join(tokens_path, 'all_tokens_' + str(counter) + '.txt') # define path to save all tokens extracted from a file in splitted_processed_tokens\n",
        "    tokens_df.to_csv(path, sep=\"|\", header=['movie_id', 'tokens'], index=None) # save tokens to file\n",
        "    save_tokens(tokens_df) # save tokens to files for each movie\n",
        "    # tokens term frequency for each movie\n",
        "    tokens_term_freq = term_freq(tokens_df, 'tokens') # calculate term frequency\n",
        "    path = os.path.join(tokens_path, 'tokens_term_freq_' + str(counter) + '.txt')\n",
        "    tokens_term_freq.to_csv(path, sep=\"|\", header=['term_freq', 'term', 'movie_id'], index=None)\n",
        "    time = strftime(\"%H:%M:%S\", gmtime())\n",
        "    print(time + ' creating tokens lists... \\n')\n",
        "    per_movie_freq_list(movie_id_list, tokens_term_freq, 'tokens', 'term_freq') # save tokens term frequency for each movie in movie_id_list\n",
        "  \n",
        "  # Bigrams extraction\n",
        "  if bigrams_extraction:\n",
        "    time = strftime(\"%H:%M:%S\", gmtime())\n",
        "    print(time + ' extracting bigrams... \\n')\n",
        "    # Define bigrams path\n",
        "    bigrams_path = os.path.join('all_movies', 'bigrams')\n",
        "    if not os.path.exists(bigrams_path):\n",
        "        os.makedirs(bigrams_path)\n",
        "    # Bigrams extraction\n",
        "    bigrams_df = extract_bigrams(processed_tokens_df) # filter and extract bigrams\n",
        "    path = os.path.join(bigrams_path, 'all_bigrams_' + str(counter) + '.txt') # define path to save all bigrams extracted from a file in splitted_processed_tokens\n",
        "    bigrams_df.to_csv(path, sep=\"|\", header=['movie_id', 'bigrams'], index=None) # save bigrams to file \n",
        "    save_bigrams(bigrams_df) # save bigrams to files for each movie\n",
        "    # Bigrams term frequency for each movie\n",
        "    bigrams_term_freq = term_freq(bigrams_df, 'bigrams')\n",
        "    path = os.path.join(bigrams_path, 'bigrams_term_freq_' + str(counter) + '.txt')\n",
        "    bigrams_term_freq.to_csv(path, sep=\"|\", header=['term_freq', 'term', 'movie_id'], index=None)\n",
        "    time = strftime(\"%H:%M:%S\", gmtime())\n",
        "    print(time + ' creating bigrams lists... \\n')\n",
        "    per_movie_freq_list(movie_id_list, bigrams_term_freq, 'bigrams', 'term_freq') # save bigrams term frequency for each movie in movie_id_list\n",
        "\n",
        "    time = strftime(\"%H:%M:%S\", gmtime())\n",
        "    print(time + ' Extraction from processed_tokens' + str(counter) + ' DONE \\n')\n",
        "\n",
        "time = strftime(\"%H:%M:%S\", gmtime())\n",
        "print(' Aspect Extraction COMPLETE \\n')\n",
        "print('===========================\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQzB-HnSlNWo"
      },
      "source": [
        "Viene eseguita l'unione di tutti i file contenenti unigrammi e bigrammi estratti e relativa term frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJVVrz4l5xN2"
      },
      "source": [
        "# join splitted dataframes resulted from aspect extraction\n",
        "join_dataframes(start, stop, 'tokens', 'all_tokens')\n",
        "join_dataframes(start, stop, 'tokens', 'tokens_term_freq')\n",
        "join_dataframes(start, stop, 'bigrams', 'all_bigrams')\n",
        "join_dataframes(start, stop, 'bigrams', 'bigrams_term_freq')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox_S2uo4olRi"
      },
      "source": [
        "## Aspect Selection\n",
        "\n",
        "Input di questa fase: liste di unigrammi e bigrammi estratti per ogni film e relativa term frequency;\n",
        "\n",
        "Output di questa fase: file per il popolamento del db di ConveRSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXd3ed3ovB4_"
      },
      "source": [
        "### Generazione lista di film da escludere\n",
        "Viene generata la lista di film da escludere.\n",
        "Per decidere quali film escludere settare i parametri tokens_delete_threshold e bigrams_delete_threshold.\n",
        "Il valore assegnato ai parametri serve ad escludere i film con un numero di unigrammi\\bigrammi al di sotto della soglia scelta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMvJr5-02jU3"
      },
      "source": [
        "#Unigrammi\n",
        "tokens_delete_threshold = 150 # escludi i film con meno di 150 unigrammi estratti\n",
        "\n",
        "df = pd.read_csv('dataset_movie_list.txt', sep='|', names=['movie_id']) # lista degli id dei film per i quali ci sono recensioni\n",
        "rpf = pd.read_csv('reviewsPerFilm.txt', sep='|', names=['movie_id', 'n_reviews']) # lista con id dei film associati al numero di recensioni\n",
        "df = df.merge(rpf, on='movie_id')\n",
        "\n",
        "movie_id_list = df['movie_id'].unique()\n",
        "\n",
        "rows_list = []\n",
        "for row in df.itertuples():\n",
        "  path = os.path.join(str(row.movie_id), str('tokens'))\n",
        "  n_tokens = 1\n",
        "  try:\n",
        "    mdf = pd.read_csv(os.path.join(path, str('extracted_tokens.txt')), sep='|', names=['tokens'])\n",
        "  except OSError as e:\n",
        "    n_tokens = 0\n",
        "\n",
        "  if n_tokens == 1:\n",
        "    n_tokens = len(mdf.index)\n",
        "\n",
        "  n_reviews = row.n_reviews\n",
        "  dict1 = {}\n",
        "  dict1.update({'movie_id': str(row.movie_id)})\n",
        "  dict1.update({'n_reviews': n_reviews})\n",
        "  dict1.update({'n_tokens': n_tokens})\n",
        "  rows_list.append(dict1)\n",
        "\n",
        "df = pd.DataFrame(rows_list)\n",
        "df.drop(df[(df.n_tokens >= 150)].index, inplace=True)\n",
        "df.sort_values(by='n_reviews', inplace=True, ascending=False)\n",
        "df.to_csv('no_tokens_list.txt', sep='|', index=None, columns=['movie_id'])\n",
        "\n",
        "# Bigrammi\n",
        "bigrams_delete_threshold = 150 # escludi i film con meno di 150 bigrammi estratti\n",
        "\n",
        "df= pd.read_csv('dataset_movie_list.txt', sep='|', names=['movie_id'])\n",
        "rpf = pd.read_csv('reviewsPerFilm.txt', sep='|', names=['movie_id', 'n_reviews'])\n",
        "df = df.merge(rpf, on='movie_id')\n",
        "\n",
        "movie_id_list = df['movie_id'].unique()\n",
        "\n",
        "rows_list = []\n",
        "for row in df.itertuples():\n",
        "  path = os.path.join(str(row.movie_id), str('bigrams'))\n",
        "  n_bigrams = 1\n",
        "  try:\n",
        "    mdf = pd.read_csv(os.path.join(path, str('extracted_bigrams.txt')), sep='|', names=['bigrams'])\n",
        "  except OSError as e:\n",
        "    n_bigrams = 0\n",
        "\n",
        "  if n_bigrams == 1:\n",
        "    n_bigrams = len(mdf.index)\n",
        "\n",
        "  n_reviews = row.n_reviews\n",
        "  dict1 = {}\n",
        "  dict1.update({'movie_id': str(row.movie_id)})\n",
        "  dict1.update({'n_reviews': n_reviews})\n",
        "  dict1.update({'n_bigrams': n_bigrams})\n",
        "  rows_list.append(dict1)\n",
        "\n",
        "df = pd.DataFrame(rows_list)\n",
        "df.drop(df[(df.n_bigrams >= 150)].index, inplace=True)\n",
        "df.sort_values(by='n_reviews', inplace=True, ascending=False)\n",
        "df.to_csv('no_bigrams_list.txt', sep='|', index=None, columns=['movie_id'])\n",
        "\n",
        "\n",
        "# Merge liste\n",
        "tok_del_list = pd.read_csv('no_tokens_list.txt', sep='|')\n",
        "big_del_list = pd.read_csv('no_bigrams_list.txt', sep='|')\n",
        "\n",
        "delete_list_df = tok_del_list.merge(big_del_list, on=\"movie_id\", how='outer')\n",
        "delete_list_df.to_csv('delete_list.txt', sep='|', index = None, header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUoTPtn10Om-"
      },
      "source": [
        "### Document Frequency e TF-IDF\n",
        "A questo punto, dato che sono stati definiti i film da escludere dall'estrazione, è possibile calcolare la Document Frequency e la TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyGM1cXl6D_J"
      },
      "source": [
        "dataset_movie_list_df = pd.read_csv('dataset_movie_list.txt', sep='|', names=['movie_id'])\n",
        "delete_list_df = pd.read_csv('delete_list.txt', sep='|', names=['movie_id'])\n",
        "movie_id_list = dataset_movie_list_df['movie_id'].unique()\n",
        "delete_list = delete_list_df['movie_id'].unique()\n",
        "movies_number = len(movie_id_list)\n",
        "print('movies_number: '+ str(movies_number) + '\\n')\n",
        "print('delete_list_number: ' + str(len(delete_list)) + '\\n')\n",
        "\n",
        "tokens_df = pd.read_csv('all_tokens_dataset.txt', sep='|')\n",
        "bigrams_df = pd.read_csv('all_bigrams_dataset.txt', sep='|')\n",
        "tokens_term_freq = pd.read_csv('tokens_term_freq_dataset.txt', sep='|')\n",
        "bigrams_term_freq = pd.read_csv('bigrams_term_freq_dataset.txt', sep='|')\n",
        "\n",
        "#drop the movies in the delete list\n",
        "for movie_id in delete_list:\n",
        "  dataset_movie_list_df.drop(dataset_movie_list_df[(dataset_movie_list_df.movie_id == str(movie_id))].index, inplace=True)\n",
        "  tokens_df.drop(tokens_df[(tokens_df.movie_id == str(movie_id))].index, inplace=True)\n",
        "  bigrams_df.drop(bigrams_df[(bigrams_df.movie_id == str(movie_id))].index, inplace=True)\n",
        "  tokens_term_freq.drop(tokens_term_freq[(tokens_term_freq.movie_id == str(movie_id))].index, inplace=True)\n",
        "  bigrams_term_freq.drop(bigrams_term_freq[(bigrams_term_freq.movie_id == str(movie_id))].index, inplace=True)\n",
        "\n",
        "movie_id_list = dataset_movie_list_df['movie_id'].unique()\n",
        "movies_number = len(movie_id_list)\n",
        "print('movies_number after removing movies from delete_list: '+ str(movies_number) + '\\n')\n",
        "\n",
        "df_tfidf(movie_id_list, tokens_df, bigrams_df, tokens_term_freq, bigrams_term_freq, movies_number) #sistemare qua dentro i bigrammi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZPyy3u26evd"
      },
      "source": [
        "### Estrazione top k aspetti\n",
        "Una volta che abbiamo Document Frequency e TF-IDF possiamo far girare questa funzione che estrarrà per ogni film i top k token e bigrammi e calcolerà l'overlapping. La funzione per la trasformazione dei \"bigrammi positivi\" viene chiamata qui dentro subito dopo aver estratto i top k bigrammi.\n",
        "Il numero di aspetti da estrarre può essere selezionato modificando il parametro k."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp2nMiBL7JVQ"
      },
      "source": [
        "k = 100\n",
        "\n",
        "top_k_and_overlapping(movie_id_list, k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYIg4nMbaavM"
      },
      "source": [
        "### Rimozione aspetti poco comuni\n",
        "Vengono infine rimossi gli aspetti \"poco comuni\", è possibile settare la soglia di popolarità necessaria alla rimozione settando il parametro popularity_threshold: in questo modo gli aspetti che compaiono in meno di tale numero di film verranno rimossi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUitoOxpabMP"
      },
      "source": [
        "popularity_threshold = 13\n",
        "\n",
        "# Unigrammi\n",
        "tok_over_df = pd.read_csv('tokens_overlapping_list_' + str(k) + '.txt', sep='|')\n",
        "tok_over_df.drop(tok_over_df[(tok_over_df.occurrences < popularity_threshold)].index, inplace=True)\n",
        "tok_over_df.reset_index()\n",
        "tok_over_df.to_csv('tokens_overlapping_list_clean.txt', sep='|', index=None)\n",
        "\n",
        "# Bigrammi\n",
        "big_over_df = pd.read_csv('bigrams_overlapping_list_' + str(k) + '.txt', sep='|')\n",
        "big_over_df.drop(big_over_df[(big_over_df.occurrences < popularity_threshold)].index, inplace=True)\n",
        "big_over_df.reset_index()\n",
        "big_over_df.to_csv('bigrams_overlapping_list_clean.txt', sep='|', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOg_kLPt5S-"
      },
      "source": [
        "### Mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL9W90AJ7VI7"
      },
      "source": [
        "Questa funzione crea i file utilizzati per popolare la Knowledge Base di ConveRSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsL0gj1U7f9g"
      },
      "source": [
        "map_aspect_terms(movie_id_list, k, map_tokens=True, map_bigrams=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyi3jN8NjHHE"
      },
      "source": [
        "### Crea lista unificata\n",
        "Unisce i file per il popolamento della Knowledge Base di ConveRSE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYghIHNCjHlg"
      },
      "source": [
        "tokens_list = pd.read_csv(\"tokens_list.txt\", sep='|', names=['id', 'term'])\n",
        "bigrams_list = pd.read_csv(\"bigrams_list.txt\", sep='|', names=['id', 'term'])\n",
        "unified_list = pd.concat([tokens_list, bigrams_list])\n",
        "unified_list.to_csv('unified_list.txt', sep='|', index=None, header=None)\n",
        "\n",
        "tokens_mapping = pd.read_csv(\"tokens_mapping.txt\", sep='|', names=['movie_id', 'property_type', 'aspect_id'])\n",
        "bigrams_mapping = pd.read_csv(\"bigrams_mapping.txt\", sep='|', names=['movie_id', 'property_type', 'aspect_id'])\n",
        "unified_mapping = pd.concat([tokens_mapping, bigrams_mapping])\n",
        "unified_mapping.to_csv('unified_mapping.txt', sep='|', index=None, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO3BYqy8QFtv"
      },
      "source": [
        "## Modifica label degli aspetti che hanno la stessa label di un film"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7CuhMSFQZas"
      },
      "source": [
        "### File per il popolamento della KB di ConveRSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z8yRk_OQfAO"
      },
      "source": [
        "# unigrammi\n",
        "tokens_list = pd.read_csv(\"tokens_list.txt\", sep='|', names=['id', 'term'])\n",
        "edited_tokens_list = edit_aspects_label(tokens_list, 'token')\n",
        "edited_tokens_list.to_csv('tokens_list.txt', index=None, header=None, sep='|')\n",
        "\n",
        "# bigrammi\n",
        "bigrams_list = pd.read_csv(\"bigrams_list.txt\", sep='|', names=['id', 'term'])\n",
        "edited_bigrams_list = edit_aspects_label(bigrams_list, 'bigram')\n",
        "edited_bigrams_list.to_csv('bigrams_list.txt', index=None, header=None, sep='|')\n",
        "\n",
        "# unigrammi e bigrammi\n",
        "tokens_list = pd.read_csv(\"tokens_list.txt\", sep='|', names=['id', 'term'])\n",
        "bigrams_list = pd.read_csv(\"bigrams_list.txt\", sep='|', names=['id', 'term'])\n",
        "unified_list = pd.concat([tokens_list, bigrams_list])\n",
        "unified_list.to_csv('unified_list.txt', sep='|', index=None, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgyfhZHQlpQn"
      },
      "source": [
        "# Conclusione\n",
        "I file necessari per il popolamento del database di ConveRSE sono: \n",
        "*   tokens_list: contiene la lista degli unigrammi estratti corredati dal loro ID;\n",
        "*   tokens_mapping: contiene la lista degli ID dei film associati agli ID degli unigrammi;\n",
        "*   bigrams_list: contiene la lista dei bigrammi estratti corredati dal loro ID;\n",
        "*   bigrams_mapping: contiene la lista degli ID dei film associati agli ID dei bigrammi;\n",
        "*   unified_list: unione di tokens_list e bigrams_list;\n",
        "*   unified_mapping: unione di tokens_mapping e bigrams_mapping;\n",
        "\n",
        "Per popolare il database di ConveRSE lanciare la classe dbPopulator.DatabasePopulator in ConveRSETools passando i seguenti argomenti:\n",
        "-c percordo del file configuration.json -p percorso della lista di aspetti da utilizzare -m percorso del file contenente gli ID dei film associati agli ID degli aspetti.\n",
        "\n",
        "Es: -c C:\\Users\\alema\\git\\MovieRecSysService\\resources\\configuration.json -p C:\\Users\\alema\\git\\unified_list.txt -m C:\\Users\\alema\\git\\unified_mapping.txt\n"
      ]
    }
  ]
}
