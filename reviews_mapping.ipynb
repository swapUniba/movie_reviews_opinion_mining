{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "reviews_mapping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2F0-k5ZrcUc"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10it1IU-rchS"
      },
      "source": [
        "# seleziona directory di lavoro\n",
        "%cd D:\\opimi_test\\ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBvwRiiagtoi"
      },
      "source": [
        "# Mapping dataset review\n",
        "## Mapping id dei film in _entities.txt_ con l'id di IMDB\n",
        "Partendo da una cartella contenente i file _entities.txt_, _entityPropertyMapping.txt_ e tutti i file del dataset con le recensioni, chiamati  _ttxxxxxxx_ (ossia con l'id IMDB del film che trattano) generiamo il file _idMapping.txt_ contenente nella prima colonna l'id del film (da _entities.txt_) e nella seconda colonna l'id di IMDB (prelevato da _entityPropertyMapping_)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iasj_9vXgtok"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "entities = pd.read_csv(\"entities.txt\", sep=\"|\", header=None, names=[\"movie_id\", \"movie_name\"])\n",
        "entities = entities.drop_duplicates(subset=\"movie_id\")\n",
        "\n",
        "entityPropertyMapping = pd.read_csv(\"entityPropertyMapping.txt\", sep=\"|\", header=None, names=[\"movie_id\", \"property_type\", \"property\"])\n",
        "imdbMapping = entityPropertyMapping.drop(entityPropertyMapping[entityPropertyMapping.property_type != \"P345\"].index) #mantieni solo le righe contenenti id IMDB identificati da \"P345\" nella seconda colonna\n",
        "imdbMapping = imdbMapping.drop_duplicates(subset=\"movie_id\")\n",
        "\n",
        "idMapping = entities.merge(imdbMapping, on=\"movie_id\")\n",
        "idMapping.to_csv(\"idMapping.txt\", sep=\"|\", header=None, index=None, columns=[\"movie_id\", \"property\"])\n",
        "idMapping = pd.read_csv(\"idMapping.txt\", sep=\"|\", header=None, names=[\"movie_id\", \"imdb_id\"])\n",
        "\n",
        "idMapping.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3gXj-yKgtom"
      },
      "source": [
        "## Mapping dei film in _idMapping.txt_ con le review\n",
        "Utilizzando il file _idMapping.txt_ appena generato mappiamo le recensioni con i file _ttxxxxxxx_ che contengono le recensioni in formato JSON valutando contestualmente quanti film abbiamo coperto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yg4n6Cwgton"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "total = unmapped = 0\n",
        "\n",
        "try:\n",
        "    idMapping = open(\"idMapping.txt\", \"r\", encoding=\"UTF8\")\n",
        "    entityReviewMapping = open(\"entityReviewMapping.txt\", \"w\", encoding=\"UTF8\")\n",
        "\n",
        "    for line_id in idMapping:\n",
        "        total += 1\n",
        "        splitted_line = line_id.split(\"|\")\n",
        "        movie_id = splitted_line[0]\n",
        "        imdb_id = splitted_line[1][:-1]\n",
        "\n",
        "        if os.path.exists(imdb_id) and os.stat(imdb_id).st_size != 0:\n",
        "            reviews_file = open(imdb_id, \"r\", encoding=\"UTF8\")\n",
        "\n",
        "            for line_re in reviews_file:\n",
        "                review = json.loads(line_re)\n",
        "                review_id = review[\"id\"]\n",
        "                review_text = review[\"text\"]\n",
        "                entityReviewMapping.write(movie_id + \"|\" + review_id + \"|\" + review_text + \"\\n\")\n",
        "            reviews_file.close()\n",
        "        else:\n",
        "            unmapped +=1\n",
        "finally:\n",
        "    idMapping.close()\n",
        "    entityReviewMapping.close()\n",
        "\n",
        "entityReviewMapping = pd.read_csv(\"entityReviewMapping.txt\", sep=\"|\", header=None, names=[\"movie_id\", \"review_id\", \"review_text\"])\n",
        "entityReviewMapping.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOnfUjAcgtoo"
      },
      "source": [
        "print(\"Number of films: \" + str(total) + \"\\n\")\n",
        "print(\"Number of mapped films: \" + str(total-unmapped) + \"\\n\")\n",
        "print(\"Number of unmapped films: \" + str(unmapped) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZk4b7Q9gtoo"
      },
      "source": [
        "## Numero di recensioni per film, minimo, massimo e media\n",
        "Utilizzando il file _entityReviewMapping.txt_ calcoliamo il numero di recensioni per ogni film, che verrà salvato in _reviewsPerFilm.txt_, e il numero minimo, massimo e medio di recensioni totali."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJwWXqLsgtop"
      },
      "source": [
        "entityReviewMapping[\"movie_id\"].value_counts().reset_index().to_csv(\"reviewsPerFilm.txt\", sep=\"|\", index=False, header=False)\n",
        "reviewsPerFilm = pd.read_csv(\"reviewsPerFilm.txt\", sep=\"|\", header=None, names=[\"movie_id\", \"n_reviews\"])\n",
        "\n",
        "reviewsPerFilm.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPwo3mBQgtoq"
      },
      "source": [
        "print(\"Minimum number of reviews: \" + str(reviewsPerFilm.n_reviews.min()))\n",
        "print(\"Maximum number of reviews: \" + str(reviewsPerFilm.n_reviews.max()))\n",
        "print(\"Mean number of reviews: \" + str(reviewsPerFilm.n_reviews.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McsxN6CPgtoq"
      },
      "source": [
        "## Lunghezza delle recensioni, minimo, massimo e media\n",
        "Utilizzando il file _entityReviewMapping.txt_ calcoliamo la lunghezza di ogni recensione che verrà salvata insieme ai relativi ID in _reviewsLength.txt_, quindi calcoliamo lunghezza minima, massima e media delle recensioni per ogni film e la salviamo in _reviewsStatistics.txt_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwP1EuRRgtor"
      },
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "entityReviewMapping[\"review_length\"] = entityReviewMapping.apply(lambda row: len(tokenizer.tokenize(str(row[\"review_text\"]))), axis=1)\n",
        "entityReviewMapping.to_csv(\"reviewsLength.txt\", sep=\"|\", index=False, header=False, columns=[\"movie_id\", \"review_id\", \"review_length\"])\n",
        "reviewsLength = pd.read_csv(\"reviewsLength.txt\", sep=\"|\", header=None, names=[\"movie_id\", \"review_id\", \"review_length\"])\n",
        "\n",
        "reviewsLength[\"min_length\"] = reviewsLength.groupby([\"movie_id\"], as_index=False)[\"review_length\"].transform(min)\n",
        "reviewsLength[\"max_length\"] = reviewsLength.groupby([\"movie_id\"], as_index=False)[\"review_length\"].transform(max)\n",
        "reviewsLength[\"mean_length\"] = reviewsLength.groupby([\"movie_id\"], as_index=False)[\"review_length\"].transform(np.mean)\n",
        "\n",
        "reviewsStatistics = reviewsLength.drop_duplicates(subset=\"movie_id\")\n",
        "\n",
        "del reviewsLength[\"min_length\"]\n",
        "del reviewsLength[\"max_length\"]\n",
        "del reviewsLength[\"mean_length\"]\n",
        "del reviewsStatistics[\"review_length\"]\n",
        "del reviewsStatistics[\"review_id\"]\n",
        "\n",
        "reviewsStatistics.to_csv(\"reviewsStatistics.txt\", sep=\"|\", index=False, header=False, columns=[\"movie_id\", \"min_length\", \"max_length\", \"mean_length\"])\n",
        "\n",
        "reviewsLength.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zohcJsJNgtos"
      },
      "source": [
        "reviewsStatistics.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au_QrSm0pfL4"
      },
      "source": [
        "# Creazione lista di ID dei film con recensioni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2QyIQ5OpiqB"
      },
      "source": [
        "entityReviewMapping = pd.read_csv(\"entityReviewMapping.txt\", sep=\"|\", header=None, names=[\"movie_id\", \"review_id\", \"review_text\"])\n",
        "del entityReviewMapping['review_id']\n",
        "del entityReviewMapping['review_text']\n",
        "entityReviewMapping.drop_duplicates(inplace = True)\n",
        "entityReviewMapping.to_csv('dataset_movie_list.txt', index=None, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_9pZx5RIZEA"
      },
      "source": [
        "# Divisione del dataset\n",
        "Il dataset va diviso in parti altrimenti diventa impossibile elaborarlo su un comune PC causa limiti di RAM e VRAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyXkT-DtIb98"
      },
      "source": [
        "reviewsPerFilm = pd.read_csv('reviewsPerFilm.txt', sep='|', header=None, names=['movie_id', 'n_reviews'])\n",
        "entityReviewMapping = pd.read_csv('entityReviewMapping.txt', sep='|', header=None, names=['movie_id', 'review_id', 'review_text'])\n",
        "\n",
        "sum = 0\n",
        "cut_indexes = []\n",
        "for row in reviewsPerFilm.itertuples():\n",
        "  sum = sum + row.n_reviews\n",
        "  if sum >= 40000:\n",
        "    cut_indexes.append(row.Index)\n",
        "    sum = 0\n",
        "del reviewsPerFilm[\"n_reviews\"]\n",
        "splitted_df = (np.split(reviewsPerFilm, cut_indexes))\n",
        "\n",
        "if not os.path.exists('splitted_dataset'):\n",
        "  os.makedirs('splitted_dataset')\n",
        "i = 0\n",
        "for df in splitted_df:\n",
        "  df = df.merge(entityReviewMapping, on=\"movie_id\")\n",
        "  df.to_csv(os.path.join('splitted_dataset', 'df' + str(i) + '.txt'), sep='|', header=None, index=None)\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}